#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Auto-updater for AI Discovery Blog.

- Fetches AI/tech articles from Newsdata
- Quality filters (removes ‚Äúpaid plans‚Äù, etc.)
- Dynamic images:
    * Uses article image (downloaded into assets/cache) when available
    * Else uses topic hero (policy/chips/markets/research/health/edu) if present
    * Else rotates among /assets/ai-hero-{1..5}.svg that exist
    * Else falls back to /assets/ai-hero.svg
- Writes Markdown posts in _posts/ with safe YAML front matter

Requires:
  pip install requests python-slugify

Env:
  NEWS_API_KEY            (your Newsdata key)
  NEWSDATA_API_KEY        (alternative name; either works)
"""

from __future__ import annotations

import os
import re
import sys
import json
import hashlib
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional
from urllib.parse import urlparse

import requests
from slugify import slugify


# ---------------- Config ----------------

API_URL = "https://newsdata.io/api/1/news"
API_KEY = os.getenv("NEWSDATA_API_KEY") or os.getenv("NEWS_API_KEY")

# Keep keywords short so the query stays < 100 chars per Newsdata rules
KEYWORDS = [
    "ai",
    "artificial intelligence",
    "machine learning",
    "openai",
    "anthropic",
    "meta ai",
    "google ai",
]

LANG = "en"
CATEGORY = "technology"
MAX_POSTS = 5               # how many new posts per run
MAX_PAGES = 3               # pagination safety

POSTS_DIR = "_posts"
ASSET_CACHE_DIR = "assets/cache"
GENERIC_FALLBACK = "/assets/ai-hero.svg"
USER_AGENT = "ai-discovery-bot/1.4 (+github actions)"

TOPIC_HEROES = {
    "policy":   "/assets/topic-policy.svg",
    "chips":    "/assets/topic-chips.svg",
    "markets":  "/assets/topic-markets.svg",
    "research": "/assets/topic-research.svg",
    "health":   "/assets/topic-health.svg",
    "edu":      "/assets/topic-edu.svg",
}

ROTATE_CANDIDATES = [
    "/assets/ai-hero-1.svg",
    "/assets/ai-hero-2.svg",
    "/assets/ai-hero-3.svg",
    "/assets/ai-hero-4.svg",
    "/assets/ai-hero-5.svg",
]

IMG_EXT_WHITELIST = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".svg"}


# ---------------- Utils ----------------

def debug(msg: str) -> None:
    print(msg, flush=True)


def clean_text(s: Optional[str]) -> str:
    if not s:
        return ""
    return re.sub(r"\s+", " ", str(s)).strip()


def ensure_dir(path: str) -> None:
    if not os.path.isdir(path):
        os.makedirs(path, exist_ok=True)


def yml_escape(s: str) -> str:
    """Escape double quotes for YAML strings used with double quotes."""
    return clean_text(s).replace('"', r'\"')


def shorten(s: str, max_len: int = 280) -> str:
    s = clean_text(s)
    if len(s) <= max_len:
        return s
    return s[: max_len - 1].rstrip() + "‚Ä¶"


def too_similar(a: str, b: str) -> bool:
    a = clean_text(a).lower()
    b = clean_text(b).lower()
    return a == b or (a and b and (a in b or b in a))


def has_low_value_markers(t: str) -> bool:
    t_low = clean_text(t).lower()
    markers = [
        "only available in paid plans",
        "subscribe to read",
        "sign in to continue",
        "under construction",
    ]
    return any(m in t_low for m in markers)


# ---------------- API ----------------

def build_query() -> str:
    """Build a query guaranteed to be within Newsdata's length limits."""
    q = " OR ".join(KEYWORDS)
    if len(q) > 95:
        q = '"artificial intelligence" OR ai OR "machine learning"'
    return q


def call_api(page: Optional[int] = None) -> Dict[str, Any]:
    params = {
        "apikey": API_KEY,
        "q": build_query(),
        "language": LANG,
        "category": CATEGORY,
    }
    if page:
        params["page"] = page

    headers = {"User-Agent": USER_AGENT}
    r = requests.get(API_URL, params=params, headers=headers, timeout=20)
    r.raise_for_status()
    return r.json()


# ---------------- Image helpers ----------------

def guess_ext_from_ct(ct: str) -> str:
    ct = (ct or "").lower()
    if "svg" in ct:
        return ".svg"
    if "webp" in ct:
        return ".webp"
    if "png" in ct:
        return ".png"
    if "gif" in ct:
        return ".gif"
    return ".jpg"


def download_and_cache_image(url: str, title: str) -> Optional[str]:
    try:
        resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=20, stream=True)
        resp.raise_for_status()

        ct = (resp.headers.get("Content-Type") or "").lower()
        if "image" not in ct:
            return None

        # name and ext
        path_name = os.path.basename(urlparse(url).path) or slugify(title)
        base, ext = os.path.splitext(path_name)
        if ext.lower() not in IMG_EXT_WHITELIST:
            ext = guess_ext_from_ct(ct)

        # unique-ish filename
        h = hashlib.sha1(url.encode("utf-8")).hexdigest()[:8]
        filename = f"{slugify(base) or 'img'}-{h}{ext}"

        ensure_dir(ASSET_CACHE_DIR)
        abs_path = os.path.join(ASSET_CACHE_DIR, filename)

        with open(abs_path, "wb") as f:
            for chunk in resp.itercontent(65536):
                if chunk:
                    f.write(chunk)

        return f"/{ASSET_CACHE_DIR}/{filename}"
    except Exception as e:
        debug(f"img download fail: {e}")
        return None


def detect_topic(title: str, desc: str) -> Optional[str]:
    t = f"{title} {desc}".lower()
    if any(k in t for k in ["policy", "regulation", "law", "ban"]):
        return "policy"
    if any(k in t for k in ["gpu", "chip", "nvidia", "amd", "hardware"]):
        return "chips"
    if any(k in t for k in ["stock", "market", "shares", "revenue", "valuation"]):
        return "markets"
    if any(k in t for k in ["research", "paper", "breakthrough", "study"]):
        return "research"
    if any(k in t for k in ["health", "medical", "doctor", "clinical"]):
        return "health"
    if any(k in t for k in ["education", "school", "classroom", "student"]):
        return "edu"
    return None


def pick_rotating_hero(title: str) -> str:
    existing = [p for p in ROTATE_CANDIDATES if os.path.exists(p.lstrip("/"))]
    if not existing:
        return GENERIC_FALLBACK
    idx = int(hashlib.md5(title.encode("utf-8")).hexdigest(), 16) % len(existing)
    return existing[idx]


def pick_image(item: Dict[str, Any], title: str, desc: str) -> str:
    # 1) original image
    for k in ("image_url", "image"):
        url = clean_text(item.get(k))
        if url and url.startswith(("http://", "https://")):
            local = download_and_cache_image(url, title)
            if local:
                return local

    # 2) topic hero
    topic = detect_topic(title, desc)
    if topic:
        candidate = TOPIC_HEROES.get(topic)
        if candidate and os.path.exists(candidate.lstrip("/")):
            return candidate

    # 3) rotating generic
    return pick_rotating_hero(title)


# ---------------- Posts ----------------

def fetch_articles(limit: int = MAX_POSTS) -> List[Dict[str, Any]]:
    if not API_KEY:
        raise ValueError("API KEY not set (NEWSDATA_API_KEY or NEWS_API_KEY).")

    debug("üß† API_URL: %s" % API_URL)
    debug("üîë Using key from env: %s" % ("NEWSDATA_API_KEY" if os.getenv("NEWSDATA_API_KEY") else "NEWS_API_KEY"))
    debug("üì∞ Fetching AI articles...")

    collected: List[Dict[str, Any]] = []
    page = 1

    while len(collected) < limit and page <= MAX_PAGES:
        try:
            data = call_api(page)
        except requests.HTTPError as e:
            debug(f"‚ùå API HTTP error on page {page}: {e}")
            break
        except Exception as e:
            debug(f"‚ùå API error on page {page}: {e}")
            break

        results = data.get("results") or data.get("articles") or []
        debug(f"üì¶ Page {page}: {len(results)} item(s)")
        if not results:
            break

        for item in results:
            if len(collected) >= limit:
                break

            title = clean_text(item.get("title"))
            desc = clean_text(item.get("description"))
            content = clean_text(item.get("content"))
            link = clean_text(item.get("link") or item.get("url"))
            source_id = clean_text(item.get("source_id") or item.get("source") or "source")
            pubdate = clean_text(item.get("pubDate") or item.get("published_at"))

            if not title or not link:
                continue
            if has_low_value_markers(title) or has_low_value_markers(desc) or has_low_value_markers(content):
                continue
            if too_similar(title, desc) and len(desc) < 60:
                continue

            image_path = pick_image(item, title, desc)

            collected.append({
                "title": title,
                "description": desc,
                "content": content,
                "link": link,
                "source_id": source_id,
                "image": image_path,
                "pubDate": pubdate or datetime.now(timezone.utc).isoformat(),
            })

        page += 1

    return collected[:limit]


def enrich_text(title: str, desc: str, content: str, source: str, link: str) -> str:
    base = clean_text(content) or clean_text(desc)
    base_short = shorten(base, 700)

    why: List[str] = []
    t = title.lower()
    if any(k in t for k in ["stock", "earnings", "valuation", "market"]):
        why.append("Impact on markets and company valuation.")
    if any(k in t for k in ["policy", "regulation", "law", "ban"]):
        why.append("Regulatory changes can reshape competition.")
    if any(k in t for k in ["chip", "gpu", "hardware", "inference"]):
        why.append("Hardware infrastructure influences AI performance and cost.")
    if any(k in t for k in ["research", "paper", "breakthrough", "study"]):
        why.append("Research advances unlock new use-cases.")
    if not why:
        why.append("Relevant for the AI ecosystem and real-world adoption.")

    bullets: List[str] = []
    for part in re.split(r"[.;]\s+", base_short)[:4]:
        part = clean_text(part)
        if part and 25 <= len(part) <= 220:
            bullets.append(part)

    parts: List[str] = []
    parts.append("### TL;DR")
    parts.append(shorten(base_short, 240) or "Quick summary of the announcement/event in AI.")
    parts.append("\n### Why it matters")
    for w in why:
        parts.append(f"- {w}")
    if bullets:
        parts.append("\n### Fast facts")
        for b in bullets:
            parts.append(f"- {b}")
    parts.append("\n> Source: " + f"[{source}]({link})")

    return "\n".join(parts).strip()


def build_markdown(article: Dict[str, Any]) -> str:
    title = article["title"]
    description = article.get("description") or ""
    body = enrich_text(title, description, article.get("content", ""), article["source_id"], article["link"])

    fm_title = yml_escape(title)
    fm_excerpt = yml_escape(shorten(description, 300))
    fm_image = article.get("image") or GENERIC_FALLBACK
    fm_source = yml_escape(article.get("source_id") or "source")
    fm_source_url = yml_escape(article.get("link") or "")
    fm_date = datetime.now(timezone.utc).date().isoformat()

    # prepare lines to avoid backslashes inside f-expressions
    fm_lines = [
        "---",
        "layout: post",
        f'title: "{fm_title}"',
        f"date: {fm_date}",
        f'excerpt: "{fm_excerpt}"',
        "categories: [ai, news]",
        f'image: "{fm_image}"',
        f'source: "{fm_source}"',
        f'source_url: "{fm_source_url}"',
        "---",
    ]

    md = "\n".join(fm_lines) + "\n\n" + body + "\n"
    return md


def make_filename(title: str, date_str: Optional[str] = None) -> str:
    date_part = (date_str or datetime.now(timezone.utc).date().isoformat())[:10]
    slug = slugify(title)[:80] or "post"
    return os.path.join(POSTS_DIR, f"{date_part}-{slug}.md")


def write_post(article: Dict[str, Any]) -> Optional[str]:
    ensure_dir(POSTS_DIR)
    path = make_filename(article["title"], article.get("pubDate"))
    if os.path.exists(path):
        debug(f"‚Ü©Ô∏é Skip (exists): {os.path.basename(path)}")
        return None
    with open(path, "w", encoding="utf-8") as f:
        f.write(build_markdown(article))
    debug(f"‚úÖ Wrote: {path}")
    return path


# ---------------- Main ----------------

def main() -> None:
    try:
        if not API_KEY:
            raise ValueError("API KEY not set (NEWSDATA_API_KEY or NEWS_API_KEY).")

        ensure_dir(ASSET_CACHE_DIR)
        # keep cache folder in repo
        gitkeep = os.path.join(ASSET_CACHE_DIR, ".gitkeep")
        if not os.path.exists(gitkeep):
            with open(gitkeep, "w", encoding="utf-8") as gk:
                gk.write("")

        articles = fetch_articles(limit=MAX_POSTS)
        if not articles:
            debug("‚ÑπÔ∏è No articles found.")
            sys.exit(0)

        created = 0
        for art in articles:
            if write_post(art):
                created += 1

        if created == 0:
            debug("‚ÑπÔ∏è Nothing new to write.")
        else:
            debug(f"üéâ Done. {created} post(s) written.")
    except Exception as e:
        debug(f"‚ùå Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
